{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import snownlp\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from textblob import TextBlob\n",
    "from snownlp import SnowNLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_correlation(col):\n",
    "    global train\n",
    "    print(f\"Correlation of '{col}': {train.corr(numeric_only=True)['is_popular'][col]}\")\n",
    "\n",
    "def groupby_class(col):\n",
    "    global train\n",
    "    dst = col\n",
    "    train[f'groupby_{col}'] = train['is_popular'].groupby(train[f'{col}']).transform('mean')\n",
    "    train[f'feature_{col}'] = train[f'groupby_{col}'].apply(lambda x : round(x, 1))\n",
    "    table = dict()\n",
    "    for i in range(len(train)):\n",
    "        table[train[f'{col}'][i]] = train[f'groupby_{col}'][i]\n",
    "\n",
    "    with open(f'./feature/temp/{dst}.pkl', 'wb') as handle:\n",
    "        pickle.dump(table, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(table)\n",
    "\n",
    "    \n",
    "    print_correlation(f'groupby_{col}')\n",
    "    print_correlation(f'feature_{col}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Read file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './dataset/label/train.pkl'\n",
    "save_data_path = './feature/train_feature.pkl'\n",
    "\n",
    "train = pd.read_pickle(train_data_path)\n",
    "save = True\n",
    "'''\n",
    "train = train.loc[train['n_comment'] != -1]\n",
    "train = train.loc[train['abstract'] != '']\n",
    "train = train.loc[train['abstract'] != ' ']\n",
    "train = train.loc[train['abstract'] != '  ']\n",
    "train = train.loc[train['headline'] != '']\n",
    "train = train.drop(index = [31840, 8580, 23040, 23365, 27631, 42070, 36259])\n",
    "train = train.reset_index().drop(columns=['index'])\n",
    "'''\n",
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Set bound of popularity</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = train['n_comment'].median()\n",
    "print(f'Median: {median}')\n",
    "train['is_popular'] = train['n_comment'].apply(lambda x : 1 if x > median else 0)\n",
    "pop_count = 0\n",
    "for i in train['is_popular']:\n",
    "    if i:\n",
    "        pop_count += 1\n",
    "print('popular:    ' , pop_count / len(train))\n",
    "print('not popular:' , 1 - pop_count / len(train))\n",
    "#train = train.drop(columns = ['n_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Hour</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.groupby('pub_time')['is_popular'].mean())\n",
    "sns.lineplot(data = train.groupby('pub_time')['is_popular'].mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (Hour)', fontsize=14)\n",
    "plt.axvline(x = 6, linestyle= '--')\n",
    "plt.axvline(x = 10, linestyle= '--')\n",
    "\n",
    "\n",
    "train['is_noon'] = train['pub_time'].apply(lambda x : 1 if (6 <= x and x <= 10) else 0)\n",
    "print_correlation('is_noon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Weekday</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.groupby('pub_weekday')['is_popular'].mean())\n",
    "sns.lineplot(data = train.groupby('pub_weekday')['is_popular'].mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (Day of Week)', fontsize=14)\n",
    "\n",
    "train['is_weekday'] = train['pub_weekday'].apply(lambda x : 1 if (x < 5) else 0)\n",
    "print_correlation('is_weekday')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Date of month</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.groupby('pub_date')['is_popular'].mean())\n",
    "sns.lineplot(data = train.groupby('pub_date')['is_popular'].mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (day)', fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Month</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.groupby('pub_month')['is_popular'].mean())\n",
    "sns.lineplot(data = train.groupby('pub_month')['is_popular'].mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (month)', fontsize=14)\n",
    "\n",
    "# train['is_lowmonth'] = train['pub_time'].apply(lambda x : 0 if (x <= 2) else 1)\n",
    "# print(train.corr()['is_popular']['is_lowmonth'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>News desk</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_class('news_desk')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Section</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_class('section_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Subsection</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_class('subsection_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Material</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_class('type_of_material')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Keyword</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Politics<font size=4>( party, congress, president )</font></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_party'] = train['keywords'].apply(lambda x: 1 if ('Democratic Party' in x or 'Republican Party' in x ) else 0)\n",
    "train['is_congress'] = train['keywords'].apply(lambda x: 1 if ('House of Representatives' in x or 'Senate' in x) else 0)\n",
    "train['is_president'] = train['keywords'].apply(lambda x: 1 if ('Trump, Donald J' in x or 'Biden, Joseph R Jr' in x)else 0)\n",
    "print(train.corr(numeric_only=True)['is_popular']['is_party'])\n",
    "print(train.corr(numeric_only=True)['is_popular']['is_congress'])\n",
    "print(train.corr(numeric_only=True)['is_popular']['is_president'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Event <font size=4>( Ukrainian_Russian_war, covid, Storming of the US Capitol )</font></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_ukrainian_russian_war'] = train['keywords'].apply(lambda x: 1 if ('War and Armed Conflicts' in x or 'Ukraine' in x ) else 0)\n",
    "train['is_covid'] = train['keywords'].apply(lambda x: 1 if 'Coronavirus (2019-nCoV)' in x else 0)\n",
    "train['is_storm'] = train['keywords'].apply(lambda x: 1 if 'Storming of the US Capitol (Jan, 2021)' in x else 0)\n",
    "print('ukrainian_russian_war     ', train.corr(numeric_only=True)['is_popular']['is_ukrainian_russian_war'])\n",
    "print('Coronavirus               ', train.corr(numeric_only=True)['is_popular']['is_covid'])\n",
    "print('Storming of the US Capitol', train.corr(numeric_only=True)['is_popular']['is_storm'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>News game<font size=4>(spelling bee, crossword puzzle)</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_spell'] = train['keywords'].apply(lambda x: 1 if 'Spelling Bee (Game)' in x else 0)\n",
    "train['is_crossword'] = train['keywords'].apply(lambda x: 1 if 'Crossword Puzzles' in x else 0)\n",
    "train['is_game'] = train['keywords'].apply(lambda x: 1 if 'Spelling Bee (Game)'  in x or 'Crossword Puzzles' in x else 0)\n",
    "print(train.corr()['is_popular']['is_spell'])\n",
    "print(train.corr()['is_popular']['is_crossword'])\n",
    "print(train.corr()['is_popular']['is_game'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Headline length</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = train['is_popular'].groupby(train['headline_len']).mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (headline_len)', fontsize=14)\n",
    "print(train.corr(numeric_only=True)['is_popular']['headline_len'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Abstract length</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = train['is_popular'].groupby(train['abstract_len']).mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (abstract_len)', fontsize=14)\n",
    "print(train.corr(numeric_only=True)['is_popular']['abstract_len'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Word count</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = train['is_popular'].groupby(train['word_count']).mean(),  label='Average Popularity')\n",
    "plt.title('Avg Popularity (word_count)', fontsize=14)\n",
    "print(train.corr(numeric_only=True)['is_popular']['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = train.groupby('abstract_dup').mean()['is_popular'],  label='Average Popularity')\n",
    "plt.title('Avg Popularity (abstract_dup)', fontsize=14)\n",
    "print(train.corr()['is_popular']['abstract_dup'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Lead paragraph length</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train.groupby('abstract_len').mean()['is_popular'])\n",
    "sns.lineplot(data = train.groupby('len_lead').mean()['is_popular'],  label='Average Popularity')\n",
    "plt.title('Avg Popularity (len_lead)', fontsize=14)\n",
    "print(train.corr()['is_popular']['len_lead'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6>Sentiment analyze</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>punctuation removal</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_list = list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "def remove_punctuation(headline):\n",
    "    for punc in punct_list:\n",
    "        if punc in headline:\n",
    "            headline = headline.replace(punc, ' ')\n",
    "    return headline\n",
    "train['clean_headline'] = train['headline'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>textblob</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"i am happy today.\"\n",
    "blob = TextBlob(text)\n",
    "# get the sentiment of the text\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb_score_polar(headline):\n",
    "    score = TextBlob(headline).sentiment[0]\n",
    "    return score\n",
    "def tb_score_subj(headline):\n",
    "    score = TextBlob(headline).sentiment[1]\n",
    "    return score\n",
    "train['sentiment_tb_polar'] = train['clean_headline'].apply(lambda x : tb_score_polar(x))#-1 ~ 1\n",
    "train['sentiment_tb_polar_abs'] = train['sentiment_tb_polar'].apply(lambda x :abs(x))#0 ~ 1\n",
    "train['sentiment_tb_subj'] = train['clean_headline'].apply(lambda x : tb_score_subj(x))#0 ~ 1\n",
    "train['sentiment_tb_pos'] = train['sentiment_tb_polar'].apply(lambda x : 1 if x > 0 else 0)#bool\n",
    "print(train.corr()['is_popular']['sentiment_tb_polar'])\n",
    "print(train.corr()['is_popular']['sentiment_tb_polar_abs'])\n",
    "print(train.corr()['is_popular']['sentiment_tb_subj'])\n",
    "print(train.corr()['is_popular']['sentiment_tb_pos'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>vader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = 'today is a sunny day!'\n",
    "analyzer = SentimentIntensityAnalyzer ()\n",
    "sentiment_dict = analyzer.polarity_scores(headline)\n",
    "print(sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment_vader_neg'] = train['clean_headline'].apply(lambda x : analyzer.polarity_scores(x)['neg'])#0 ~ 1\n",
    "train['sentiment_vader_neu'] = train['clean_headline'].apply(lambda x : analyzer.polarity_scores(x)['neu'])#0 ~ 1\n",
    "train['sentiment_vader_pos'] = train['clean_headline'].apply(lambda x : analyzer.polarity_scores(x)['pos'])#0 ~ 1\n",
    "train['sentiment_vader_compound'] = train['clean_headline'].apply(lambda x : analyzer.polarity_scores(x)['compound'])#-1 ~ 1\n",
    "train['sentiment_vader_compound_abs'] = train['sentiment_vader_compound'].apply(lambda x : abs(x))#0 ~ 1\n",
    "print(train.corr()['is_popular']['sentiment_vader_neg'])\n",
    "print(train.corr()['is_popular']['sentiment_vader_neu'])\n",
    "print(train.corr()['is_popular']['sentiment_vader_pos'])\n",
    "print(train.corr()['is_popular']['sentiment_vader_compound'])\n",
    "print(train.corr()['is_popular']['sentiment_vader_compound_abs'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>snownlp</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = 'i am happy'\n",
    "sentence = 'hello\",\"why is it not working?!'\n",
    "s = SnowNLP(headline)\n",
    "print(s.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment_snow'] = train['clean_headline'].apply(lambda x : SnowNLP(x).sentiments)#0 ~ 1\n",
    "train['sentiment_snow_abs'] = train['sentiment_snow'].apply(lambda x : 2 * abs(x - 0.5))#-1 ~ 1\n",
    "train['sentiment_snow_pos'] = train['sentiment_snow'].apply(lambda x : 1 if x > 0.5 else 0)#bool\n",
    "print(train.corr()['is_popular']['sentiment_snow'])\n",
    "print(train.corr()['is_popular']['sentiment_snow_abs'])\n",
    "print(train.corr()['is_popular']['sentiment_snow_pos'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6>abstract</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['clean_abstract'] = train['abstract'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>textblob</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb_score_polar(headline):\n",
    "    score = TextBlob(headline).sentiment[0]\n",
    "    return score\n",
    "def tb_score_subj(headline):\n",
    "    score = TextBlob(headline).sentiment[1]\n",
    "    return score\n",
    "train['sentiment_abstract_tb_polar'] = train['clean_abstract'].apply(lambda x : tb_score_polar(x))#-1 ~ 1\n",
    "train['sentiment_abstract_tb_polar_abs'] = train['sentiment_abstract_tb_polar'].apply(lambda x :abs(x))#0 ~ 1\n",
    "train['sentiment_abstract_tb_subj'] = train['clean_abstract'].apply(lambda x : tb_score_subj(x))#0 ~ 1\n",
    "train['sentiment_abstract_tb_pos'] = train['sentiment_abstract_tb_polar'].apply(lambda x : 1 if x > 0 else 0)#bool\n",
    "print(train.corr()['is_popular']['sentiment_abstract_tb_polar'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_tb_polar_abs'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_tb_subj'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_tb_pos'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>vader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment_abstract_vader_neg'] = train['clean_abstract'].apply(lambda x : analyzer.polarity_scores(x)['neg'])#0 ~ 1\n",
    "train['sentiment_abstract_vader_neu'] = train['clean_abstract'].apply(lambda x : analyzer.polarity_scores(x)['neu'])#0 ~ 1\n",
    "train['sentiment_abstract_vader_pos'] = train['clean_abstract'].apply(lambda x : analyzer.polarity_scores(x)['pos'])#0 ~ 1\n",
    "train['sentiment_abstract_vader_compound'] = train['clean_abstract'].apply(lambda x : analyzer.polarity_scores(x)['compound'])#-1 ~ 1\n",
    "train['sentiment_abstract_vader_compound_abs'] = train['sentiment_vader_compound'].apply(lambda x : abs(x))#0 ~ 1\n",
    "print(train.corr()['is_popular']['sentiment_abstract_vader_neg'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_vader_neu'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_vader_pos'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_vader_compound'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_vader_compound_abs'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>snownlp</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment_abstract_snow'] = train['clean_abstract'].apply(lambda x : SnowNLP(x).sentiments)#0 ~ 1\n",
    "train['sentiment_abstract_snow_abs'] = train['sentiment_abstract_snow'].apply(lambda x : 2 * abs(x - 0.5))#-1 ~ 1\n",
    "train['sentiment_abstract_snow_pos'] = train['sentiment_abstract_snow'].apply(lambda x : 1 if x > 0.5 else 0)#bool\n",
    "print(train.corr()['is_popular']['sentiment_abstract_snow'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_snow_abs'])\n",
    "print(train.corr()['is_popular']['sentiment_abstract_snow_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['web_url', 'print_section', 'print_page', 'pub_year',\n",
    "       'abstract','abstract_dup', 'headline', \n",
    "       'headline_dup', 'headline_kicker', 'pub_weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_list = train.corr()['is_popular']\n",
    "fea_list = fea_list.sort_values(key = lambda x : abs(x), ascending=False)\n",
    "print(fea_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suit_attribute = []\n",
    "for i in range(40):\n",
    "    suit_attribute.append(fea_list.index[i])\n",
    "for col in train.columns:\n",
    "    if col not in suit_attribute:\n",
    "        train = train.drop(columns = col)\n",
    "        print(col)\n",
    "        \n",
    "#print(list.columns)\n",
    "#list.sort_values(list, key = lambda x: x[1])\n",
    "#print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.corr()['is_popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(train.corr(), cmap='coolwarm', annot=False, square=True, fmt='.2f', cbar=True, center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns = 'n_comment')\n",
    "'''\n",
    "train = train.drop(columns=['group_by_news_desk', 'group_by_section_name', 'group_by_subsection_name',\n",
    "                            'group_by_type_of_material', 'feature_news_desk', 'feature_subsection_name',\n",
    "                           'sentiment_abstract_snow', 'sentiment_abstract_vader_compound', 'sentiment_vader_compound', \n",
    "                           'sentiment_abstract_vader_neu', 'is_game',  'sentiment_abstract_tb_pos'])\n",
    "train.columns\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(train.corr(), cmap='coolwarm', annot=False, square=True, fmt='.2f', cbar=True, center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "sns.heatmap(train.corr()[['is_popular']].sort_values(ascending=False, by='is_popular'), \n",
    "            cmap='coolwarm', annot=True, vmax=0.8, center=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    train.to_pickle(f'./feature/{feat_data}.pkl')\n",
    "feat = pd.read_pickle(f'./feature/{feat_data}.pkl')\n",
    "feat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index([ 'print_section', 'print_page', 'pub_year',\n",
    "       'abstract','abstract_dup', 'headline', \n",
    "       'headline_dup', 'headline_kicker', 'len_lead', \n",
    "       'is_popular'],\n",
    "      dtype='object')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
